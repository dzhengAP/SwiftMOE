% SOTA Comparison Table for ICML Paper
% Auto-generated from benchmark results

\begin{table*}[t]
\centering
\caption{SOTA comparison on 8Ã— V100 GPUs (128 experts, $d_{\text{model}}=1024$). All values in milliseconds (mean $\pm$ std). Speedup computed relative to PyTorch baseline.}
\label{tab:sota_comparison}
\begin{tabular}{lcccccc}
\toprule
\textbf{Framework} & \textbf{512} & \textbf{1024} & \textbf{2048} & \textbf{4096} & \textbf{8192} & \textbf{Mean Speedup} \\
\midrule
PyTorch (Baseline) & 6.24 $\pm$ 0.91 & 6.41 $\pm$ 1.20 & 6.74 $\pm$ 1.42 & 7.39 $\pm$ 2.16 & 8.03 $\pm$ 1.75 & 1.00$\times$ \\
DeepSpeed-MoE & 6.63 $\pm$ 1.16 & 7.94 $\pm$ 1.11 & 8.03 $\pm$ 2.41 & 8.61 $\pm$ 2.55 & 9.04 $\pm$ 1.96 & 0.87$\times$ \\
Megatron-LM & 7.99 $\pm$ 4.08 & 9.34 $\pm$ 2.64 & 9.32 $\pm$ 1.67 & 10.36 $\pm$ 3.87 & 13.24 $\pm$ 5.64 & 0.70$\times$ \\
\textbf{UltimateMoE } & 3.60 $\pm$ 1.07 & 3.60 $\pm$ 1.72 & 3.96 $\pm$ 2.01 & 4.37 $\pm$ 3.90 & 5.25 $\pm$ 2.17 & \textbf{1.69$\times$} \\
\bottomrule
\end{tabular}
\end{table*}